# Configuration file for Relationship Dynamics Simulator

# Environment Configuration
environment:
  max_episode_steps: 20  # Recommended: 12-20 steps per episode
  history_length: 10
  use_history: false  # Set to true for deep RL
  
  # Initial state values
  initial_emotion: -0.3  # Range: [-1, 1], Recommended: -0.4 to -0.2 for conflict scenarios
  initial_trust: 0.5  # Range: [0, 1], Recommended: 0.4 to 0.6
  initial_calmness_a: 0.4  # Range: [0, 1], Recommended: 0.3 to 0.5
  initial_calmness_b: 0.4  # Range: [0, 1], Recommended: 0.3 to 0.5
  
  # Irritability traits (fixed personality)
  irritability_a: 0.4  # Range: [0, 1]
    # 0.7: Impulsive type (high reactivity)
    # 0.4: Neutral (moderate reactivity)
    # 0.2: Stable type (low reactivity)
  irritability_b: 0.4  # Range: [0, 1]
  
  # State update parameters
  recovery_rate: 0.02  # Automatic calmness recovery per step
  
  # Action feasibility parameters
  feasibility_alpha: 1.0  # Weight for calmness in feasibility calculation
  feasibility_beta: 1.0  # Weight for action difficulty
  
  # Reward weights
  reward_weights:
    emotion: 1.0
    trust: 1.0
    action_bonus: 0.1
  
  # State clipping ranges
  emotion_clip_range: [-1, 1]
  trust_clip_range: [0, 1]
  calmness_clip_range: [0, 1]
  
  # Termination conditions (updated for Deep RL optimization)
  termination:
    success_emotion_threshold: 0.2  # Moderate repair (from negative to slightly positive)
    success_trust_threshold: 0.6  # Moderate trust recovery
    failure_emotion_threshold: -0.9  # Extreme conflict
    failure_trust_threshold: 0.1  # Very low trust

# Personality Configuration (legacy, for reference)
personalities:
  agent_a: neutral  # Options: neutral, impulsive, sensitive, avoidant
  agent_b: neutral

# Shallow RL Configuration
shallow_rl:
  algorithm: q_learning  # Options: q_learning, sarsa
  num_actions: 10
  state_bins: 5
  learning_rate: 0.1
  discount_factor: 0.95
  epsilon: 0.1
  epsilon_decay: 0.995
  epsilon_min: 0.01

# Deep RL Configuration
deep_rl:
  algorithm: dqn  # Options: dqn, ppo
  learning_rate: 3e-4  # DQN: 3e-4 (optimized for conflict resolution)
  discount_factor: 0.99  # Increased for long-term relationship repair
  epsilon: 1.0  # DQN only
  epsilon_decay: 0.997  # DQN only - slower decay for more exploration
  epsilon_min: 0.05  # DQN only - higher min for emotionally unstable behavior
  batch_size: 64
  memory_size: 20000  # DQN only - larger buffer
  target_update_freq: 200  # DQN only - less frequent updates for stability
  # PPO-specific
  gae_lambda: 0.95
  clip_epsilon: 0.2
  value_coef: 0.5
  entropy_coef: 0.01
  max_grad_norm: 0.5
  ppo_epochs: 4

# Training Configuration
training:
  num_episodes: 8000  # Deep RL: 8000 episodes
  train_mode: self_play  # Options: self_play, fixed_opponent
  log_interval: 200  # Deep RL: log every 200 episodes
  save_interval: 2000  # Deep RL: save every 2000 episodes
  save_dir: ./checkpoints

# Evaluation Configuration
evaluation:
  num_episodes: 100
  render: false

# Data Configuration
data:
  dailydialog_path: null  # Path to DailyDialog dataset
  empathetic_dialogues_path: null  # Path to EmpatheticDialogues dataset
  use_data_calibration: false  # Whether to calibrate transition model from data
