# Configuration file for Relationship Dynamics Simulator

# Environment Configuration
environment:
  max_episode_steps: 50  # Maximum steps per episode (neutral/stalemate termination)
  history_length: 10
  use_history: false  # Set to true for deep RL
  
  # Initial state values
  initial_emotion: -0.3  # Range: [-1, 1], Recommended: -0.4 to -0.2 for conflict scenarios
  initial_trust: 0.5  # Range: [0, 1], Recommended: 0.4 to 0.6
  initial_calmness_a: 0.4  # Range: [0, 1], Recommended: 0.3 to 0.5
  initial_calmness_b: 0.4  # Range: [0, 1], Recommended: 0.3 to 0.5
  
  # Irritability traits (fixed personality)
  irritability_a: 0.4  # Range: [0, 1]
    # 0.7: Impulsive type (high reactivity)
    # 0.4: Neutral (moderate reactivity)
    # 0.2: Stable type (low reactivity)
  irritability_b: 0.4  # Range: [0, 1]
  
  # State update parameters
  recovery_rate: 0.02  # Automatic calmness recovery per step
  cross_agent_calmness_factor: 0.6  # Factor for cross-agent calmness influence
                                     # When agent A takes an action, agent B's calmness
                                     # is affected by delta_calmness * this_factor
                                     # (default: 0.6 means 60% of the effect)
  
  # Action feasibility parameters
  feasibility_alpha: 1.0  # Weight for calmness in feasibility calculation
  feasibility_beta: 1.0  # Weight for action difficulty
  
  # Reward weights
  reward_weights:
    emotion: 1.0
    trust: 1.0
    action_bonus: 0.1
  
  # State clipping ranges
  emotion_clip_range: [-1, 1]
  trust_clip_range: [0, 1]
  calmness_clip_range: [0, 1]
  
  # Termination conditions (updated for better training signals)
  termination:
    success_emotion_threshold: 0.4  # Moderate repair (emotion > 0.4)
    success_trust_threshold: 0.6  # Moderate trust recovery (trust > 0.6)
    failure_emotion_threshold: -0.5  # Extreme conflict (emotion < -0.5)
    failure_trust_threshold: 0.1  # Very low trust (trust < 0.1)

# Personality Configuration (legacy, for reference)
personalities:
  agent_a: neutral  # Options: neutral, impulsive, sensitive, avoidant
  agent_b: neutral

# Shallow RL Configuration
shallow_rl:
  algorithm: q_learning  # Options: q_learning, sarsa
  num_actions: 10
  state_bins: 5
  learning_rate: 0.1
  discount_factor: 0.95
  epsilon: 0.1
  epsilon_decay: 0.995
  epsilon_min: 0.01

# Deep RL Configuration
deep_rl:
  algorithm: dqn  # Options: dqn, ppo
  learning_rate: 3e-4  # DQN: 3e-4 (optimized for conflict resolution)
  discount_factor: 0.99  # Increased for long-term relationship repair
  epsilon: 1.0  # DQN only
  epsilon_decay: 0.997  # DQN only - slower decay for more exploration
  epsilon_min: 0.05  # DQN only - higher min for emotionally unstable behavior
  batch_size: 64
  memory_size: 20000  # DQN only - larger buffer
  target_update_freq: 200  # DQN only - less frequent updates for stability
  # PPO-specific
  gae_lambda: 0.95
  clip_epsilon: 0.2
  value_coef: 0.5
  entropy_coef: 0.01
  max_grad_norm: 0.5
  ppo_epochs: 4

# Training Configuration
training:
  num_episodes: 4000  # Deep RL: 4000 episodes
  train_mode: self_play  # Options: self_play, fixed_opponent
  log_interval: 200  # Deep RL: log every 200 episodes
  save_interval: 2000  # Deep RL: save every 2000 episodes
  save_dir: ./checkpoints

# Evaluation Configuration
evaluation:
  num_episodes: 100
  render: false

# Data Configuration
data:
  dailydialog_path: null  # Path to DailyDialog dataset
  empathetic_dialogues_path: null  # Path to EmpatheticDialogues dataset
  use_data_calibration: false  # Whether to calibrate transition model from data
 
# Sampling & Reproducibility (Beta sampling parameters and repeat counts)
sampling:
  positive_action_alpha: 3.0  # Beta alpha for positive actions (bias toward upper part of interval)
  positive_action_beta: 2.0   # Beta beta for positive actions
  negative_action_alpha: 2.0  # Beta alpha for negative actions (bias toward lower part of interval)
  negative_action_beta: 3.0   # Beta beta for negative actions
  mixed_action_alpha: 2.5     # Beta alpha for intervals that cross zero
  mixed_action_beta: 2.5      # Beta beta for intervals that cross zero
  shallow_repeats: 10         # Number of repeats for Shallow RL experiments
  deep_repeats: 15            # Number of repeats for Deep RL experiments
  base_seed: 42               # Base random seed; runs use base_seed + run_index