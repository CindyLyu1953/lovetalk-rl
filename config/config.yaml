# Configuration file for Relationship Dynamics Simulator

# Environment Configuration
environment:
  max_episode_steps: 20
  history_length: 10
  use_history: false  # Set to true for deep RL
  initial_emotion: -0.3
  initial_trust: 0.6
  initial_conflict: 0.7
  reward_weights:
    emotion: 1.0
    trust: 1.0
    conflict: 1.0
    action_bonus: 0.1

# Personality Configuration
personalities:
  agent_a: neutral  # Options: neutral, impulsive, sensitive, avoidant
  agent_b: neutral

# Shallow RL Configuration
shallow_rl:
  algorithm: q_learning  # Options: q_learning, sarsa
  num_actions: 10
  state_bins: 5
  learning_rate: 0.1
  discount_factor: 0.95
  epsilon: 0.1
  epsilon_decay: 0.995
  epsilon_min: 0.01

# Deep RL Configuration
deep_rl:
  algorithm: dqn  # Options: dqn, ppo
  learning_rate: 1e-4  # DQN: 1e-4, PPO: 3e-4
  discount_factor: 0.95
  epsilon: 1.0  # DQN only
  epsilon_decay: 0.995  # DQN only
  epsilon_min: 0.01  # DQN only
  batch_size: 64
  memory_size: 10000  # DQN only
  target_update_freq: 100  # DQN only
  # PPO-specific
  gae_lambda: 0.95
  clip_epsilon: 0.2
  value_coef: 0.5
  entropy_coef: 0.01
  max_grad_norm: 0.5
  ppo_epochs: 4

# Training Configuration
training:
  num_episodes: 5000
  train_mode: self_play  # Options: self_play, fixed_opponent
  log_interval: 100
  save_interval: 1000
  save_dir: ./checkpoints

# Evaluation Configuration
evaluation:
  num_episodes: 100
  render: false

# Data Configuration
data:
  dailydialog_path: null  # Path to DailyDialog dataset
  empathetic_dialogues_path: null  # Path to EmpatheticDialogues dataset
  use_data_calibration: false  # Whether to calibrate transition model from data
